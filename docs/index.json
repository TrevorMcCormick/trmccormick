[{"categories":null,"content":"Trevor McCormick","date":"2021-08-01","objectID":"/about/","tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":"This website was designed to publish thoughts, how-to‚Äôs, resources, notes, and everything in-between in the general realm of data products, but also whatever comes to mind. I live in Wyandotte, Michigan with my wife, Haley, and our dog, Captain. I‚Äôm working as a Product Analyst for Disney Streaming, working within the Data Solutions \u0026 Instrumentation team to design and implement tagging across all of Disney‚Äôs streaming products. ","date":"2021-08-01","objectID":"/about/:0:0","tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":"The Difference Between Synchronous and Asynchronous Loading After you type in the URL to a website, some administrative magic happens behind the scenes. Here is a sketch of my entire website build, with the browser‚Äôs request and download happening on the left side of the image: When you load a website, you can use Chrome‚Äôs DevTools to view how your browser is downloading, organizing, and delivering content from a website to your browser. Here is a quick snippet of the Network tab, with my cache disabled to show what a full request looks like: As you can see from the Waterfall on the right side of the image, the types of data are being downloaded in the following order: document, script, sytlesheet, font. The 200 status lets your browser know that the file has been downloaded successfully, and the Initiator column lets you know from where that file originated. There are some other helpful columns, such as size of the file, and how many milliseconds it takes for your browser to download and render that data. Once your browser starts a connection with the web server that hosts a website, it first tries to download the index.html file, which basically is a set of instructions telling it how to render the page you‚Äôre asking for. In my site, for example, it has some Javascript files that are hosted in some folder on the webserver, and it has some CSS stylesheets that are hosted in another folder. Once you have downloaded everything from my webserver, then it fetches some other files that are hosted on other webservers. In this instance, the only thing that my site is asking you to load are font files, hosted in two different locations: fonts.googleapis.com, and font awesome‚Äôs CDN. You can see that at this point, the fonts are not loading asynchronously; they start downloading after the files from my webserver are already downloaded. ","date":"2021-07-08","objectID":"/drafts/asynchronous_assets/:1:0","tags":["UX"],"title":"Loading Fonts and Icons Asynchronously","uri":"/drafts/asynchronous_assets/"},{"categories":null,"content":"How to host fonts locally instead of through a CDN or an API In order to asynchronously load our fonts, let‚Äôs work with Google fonts first. We need to go download the woff files from Google, then put it somewhere on our site locally. Here is an amazing resource that will help you generate the correct CSS code to host your fonts locally, and package up the right files for your webserver. If you‚Äôre hosting a Hugo blog, here is the process I went through: Download the correct fonts from the above link. Package them up in a directory called fonts. I put mine in the public/styles directory. Create a new section in public/dist/site.css and add the CSS snippet from the above link. If you were to reload your site at this point in localhost, you would see the following change, indicating that the Google fonts are now being hosted locally, so they start loading earlier. However, the site still has some code that is telling the browser to also go download the fonts from the Google fonts server. To take out the last call to Google Fonts, you need to delete the reference to the Google fonts in your footer.html file. Once you do that, you‚Äôll see those calls go away, and your page speed increase: So in completing these four steps, we‚Äôve done two things: we‚Äôve minimized the user‚Äôs browser‚Äôs workload, and we‚Äôve improved the user experience by delivering content to their screen faster (in these screenshots, it looks like it improved it by quite a bit, but we‚Äôll check with Google Page Insights to make sure.) The last thing we are going to do is just repeat the process for the font-awesome fonts. They have a great tutorial on how to host their icons locally. This was kind of a pain, taking about an hour to sort things out. But here is a quick overview of what I did different for font-awesome: In my config.yaml file, I added a section under params, so it looks like: params:custom_css:[\"font-awesome/font-awesome.css\"]custom_js:[\"font-awesome/font-awesome.js\"] In my static folder, I added a new folder called font-awesome, and I placed 3 things in it from the above font-awesome link: all.css (renamed to font-awesome.css) Within this file, I edited the links to the actual SVG files so that they would match where I put them in the static directory brands.js (renamed to font-awesome.js) I think at this time I only use font-awesome for the brands that are free. At some point I‚Äôd like to pay for the Pro version, but $99/year is too steep for icon hoarding üôÇ the webfonts directory (this contains all of the SVG paths that draw the icons) In the extra-in-head.html partial, I added the following code, which lets Hugo know how to use the new site parameters I just declared. {{ range .Site.Params.custom_css -}} \u003clink rel=\"stylesheet\" href=\"{{ . | absURL }}\"\u003e {{- end }} {{ range .Site.Params.custom_js -}} \u003cscript defer src=\"{{ . | absURL }}\"\u003e\u003c/script\u003e {{- end }} Finally, I had to take a look at the font-awesome.css file and figure out that the CSS class for my brand icons was fab instead of fa, so I had to go into my header.html file to edit the classes. ","date":"2021-07-08","objectID":"/drafts/asynchronous_assets/:2:0","tags":["UX"],"title":"Loading Fonts and Icons Asynchronously","uri":"/drafts/asynchronous_assets/"},{"categories":null,"content":"What speed improvements you should see Now, my site loads much faster for users because it is not wasting time going to other webservers. Here is the final screenshot of my Chrome network tab when I load it locally: And here is the result in Google Page Speed Insights: ","date":"2021-07-08","objectID":"/drafts/asynchronous_assets/:3:0","tags":["UX"],"title":"Loading Fonts and Icons Asynchronously","uri":"/drafts/asynchronous_assets/"},{"categories":["Certifications"],"content":"Background There is no better preparation than hands-on experience with AWS. Before I attempted this certification exam, I had about two years of experience working across AWS, primarily using services to build my website and handle data ingestion and warehousing processes at work. I had drawn out some simple architecture documents up to this point, and I could generally describe what was happening. However, I knew that in order to pass the exam, I would need to strengthen my understanding of services I was already familiar with (S3, RDS, IAM), and spend a significant amount of time learning the ones I did not use day to day (VPC, Route53, Networking, etc). Here is a link to the exam guide which identifies the four content domains you must know in order to pass: resiliency, high-performance, security, and cost-optimization. When you study for this exam, if you‚Äôre reading about a specific service, or building something out in the console, try to understand the service offering with these four concepts in mind. Some key questions to ask yourself might be: what happens if this service fails? how can I scale my application to meet demand? how can I lockdown the components of my VPC so that it is protected from internal and external threats? is this the most cost-effective way to achieve my goals? ","date":"2021-06-02","objectID":"/aws_certified_solutions_architect_associate/:1:0","tags":["AWS"],"title":"How to Study for the AWS Certified Solutions Architect Associate Exam","uri":"/aws_certified_solutions_architect_associate/"},{"categories":["Certifications"],"content":"A Cloud Guru I felt like the first best step for me would be to get through all of the videos and exercises in A Cloud Guru‚Äôs - AWS Certified Solutions Architect Associate SAA-C02 course. There was 45 hours of content in this course alone, so it was definitely a big first step. As I was going through this course, I browsed through some of the other offerings, and found the [AWS Well-Architected Framework course]](https://acloud.guru/overview/aws-well-architected-framework) fairly helpful. What I found MOST helpful was the hands-on labs. Here is a curated list of labs I found most helpful: ALBs and Auto-Scaling AMIs Bastion Hosts Database Migration EBS Volumes RDS SQS VPCs ","date":"2021-06-02","objectID":"/aws_certified_solutions_architect_associate/:2:0","tags":["AWS"],"title":"How to Study for the AWS Certified Solutions Architect Associate Exam","uri":"/aws_certified_solutions_architect_associate/"},{"categories":["Certifications"],"content":"Other Resources After taking the two ACG courses and moving through these labs, I used the following resources to prepare for the ACG practice exams: AWS Cheat Sheets by Tutorials Dojo AWS FAQs AWS 200-level Hands-On Tutorials AWS Reference Architecture Diagrams After all of this preparation, my wife basically told me I needed to stop studying and just take the exam üòÖ So that‚Äôs what I did! ","date":"2021-06-02","objectID":"/aws_certified_solutions_architect_associate/:3:0","tags":["AWS"],"title":"How to Study for the AWS Certified Solutions Architect Associate Exam","uri":"/aws_certified_solutions_architect_associate/"},{"categories":["Certifications"],"content":"Ta-da I passed the test without much trouble, flagging only a few questions for review. My advice after seeing the questions on the test is that AWS truly provides the best resources, but the labs from A Cloud Guru are vital. You really have to build a VPC from scratch in your head to answer the long-form questions on the exam. Although I do not think I‚Äôm cut out for the Solutions Architect Professional exam, I do want to continue the learning path I‚Äôm on through ACG, and potentially take the Data and Analytics specialty exam in the next couple of years. ","date":"2021-06-02","objectID":"/aws_certified_solutions_architect_associate/:4:0","tags":["AWS"],"title":"How to Study for the AWS Certified Solutions Architect Associate Exam","uri":"/aws_certified_solutions_architect_associate/"},{"categories":["Data"],"content":"In this post I‚Äôll walk through: how to create a public PostgreSQL instance in RDS using free-tier how to create a table and load it with data from a csv file how to query data from that table using psycopg2 ","date":"2021-02-04","objectID":"/aws_rds_postgres_python/:0:0","tags":["Python","AWS"],"title":"How To Query PostgreSQL in AWS RDS with Python","uri":"/aws_rds_postgres_python/"},{"categories":["Data"],"content":"PostgreSQL on RDS The only thing you have to do in the AWS console is create a Postgres DB instance and make sure it is open to the public (just for this example). Here is how to do that: Go to Databases in RDS, and choose the region you want to create a database instance Create a database, selecting ‚ÄúStandard Create‚Äù, and the PostgreSQL configuration. Make sure to use free-tier. You can name the database anything you want, and choose a username and password. The most important step is in ‚ÄúConnectivity‚Äù: make sure to fill in the bubble for ‚ÄúYes‚Äù to Public Access. If you‚Äôre using this database for any real-life work, then you‚Äôll want to fill in ‚ÄúNo‚Äù. You‚Äôll have to do some work to configure security groups and look at your architecture to only allow connections you want to approve if that is the case. Once the database has been created, you‚Äôll be able to find the database endpoint in the ‚ÄúConnectivity \u0026 security‚Äù section. You‚Äôll use that to create a json file with your credentials, which should look something like this: { \"user\":\"postgres\", \"password\":\"password\", \"database\":\"postgres\", \"host\":\"xxxx.xxxxxxx.us-east-2.rds.amazonaws.com\" } ","date":"2021-02-04","objectID":"/aws_rds_postgres_python/:1:0","tags":["Python","AWS"],"title":"How To Query PostgreSQL in AWS RDS with Python","uri":"/aws_rds_postgres_python/"},{"categories":["Data"],"content":"Query with Python So you‚Äôve set up a Postgres DB instance, but there is no data in it. We‚Äôll need to connect to the instance and load data with Python. To go forward with this exercise, you‚Äôll need pip, and you‚Äôll need to install the follwing packages: psycopg2-binary, pandas, and sqlalchemy. I‚Äôve provided some example functions that you can use to get started. Here is a quick summary of the sections, with the actual python code at the bottom of the post. Import: so you can skip a bunch of database driver steps Client: to connect to the psql instance for queries Load: to load data into psql. I only put one function in this class for an example, so you can create and load a table in one step. Query: to query data in a table within your DB instance Meta: to inspect the DB instance ","date":"2021-02-04","objectID":"/aws_rds_postgres_python/:2:0","tags":["Python","AWS"],"title":"How To Query PostgreSQL in AWS RDS with Python","uri":"/aws_rds_postgres_python/"},{"categories":["Data"],"content":"Google Colab Most of my ad-hoc work is done in Google Colab because it‚Äôs easy to run code blocks and debug interactively. I‚Äôm going to share an example Colab Notebook with you so you should be up and running fast. Here is the link to the Python Notebook that you can upload to your own Colab environment. In the Google Colab environment, you need to upload two files: your credentials json file, and your dataset. In this case, I‚Äôve downloaded the iris dataset and I will upload it to my DB instance as the table iris. ","date":"2021-02-04","objectID":"/aws_rds_postgres_python/:3:0","tags":["Python","AWS"],"title":"How To Query PostgreSQL in AWS RDS with Python","uri":"/aws_rds_postgres_python/"},{"categories":["Data"],"content":"Video Walkthrough Below is a screen recording of me going through the Colab process. I‚Äôm uploading two files, and running through all the code blocks to connect to my DB instance and work with it. And here is the Python code you can copy and try out for yourself: ","date":"2021-02-04","objectID":"/aws_rds_postgres_python/:3:1","tags":["Python","AWS"],"title":"How To Query PostgreSQL in AWS RDS with Python","uri":"/aws_rds_postgres_python/"},{"categories":["Certifications"],"content":"Cloud Foundations The AWS Certified Cloud Practitioner Exam assumes you have some familiarity with cloud concepts. If you‚Äôre completely unfamiliar with those, it may benefit you to learn or review some introductory computing concepts. When I was in college, I completed two courses that gave me a great foundation: Stanford CS 101 - Introduction to Computing Principles Harvard CS50 - Introduction to Computer Science This course features one of my favorite professors of all time ‚Äì David Malan. Once you‚Äôre finished with these courses, you will be able to explain how computers work, how the Internet works, and you‚Äôll have a great start working with programming languages. Most importantly, you will be ready to learn how the cloud works. It‚Äôs not necessary to complete these courses formally, but it would help to browse through the lessons and build up knowledge on any foreign topics. ","date":"2020-10-04","objectID":"/aws_certified_cloud_practitioner/:1:0","tags":["AWS"],"title":"How to Study for the AWS Certified Cloud Practitioner Exam","uri":"/aws_certified_cloud_practitioner/"},{"categories":["Certifications"],"content":"Guided Training There are two cloud training platforms I will recommend for this certification: A Cloud Guru and Cloud Academy. A Cloud Guru - AWS Certified Cloud Practitioner 2020 Designed for the non-technical crowd, this course provides 10 hours of videos, several quizzes along the way, and a final practice exam. The instructor Ryan Kroonenburg is phenomenal. I used this platform exclusively, along with the whitepapers below, and I passed my exam. A Cloud Guru‚Äôs current pricing is $32/month, and it includes access to all learning paths. I‚Äôve found it extremely worthwhile, and I‚Äôve been fortunate to use some education reimbursement funds from my employer to cover the cost for the full year. There is also a Personal Plus plan, which I have not used, but I do see the benefit in having your own managed cloud sandbox. Cloud Academy - AWS Certified Cloud Practitioner Preparation Another cloud platform out of London, this is a popular learning recommendation. I have no personal experience with this platform, but you can check out their free 7-day trial to see if you like it better than A Cloud Guru. Their pricing is about 50% more expensive at $49/month. ","date":"2020-10-04","objectID":"/aws_certified_cloud_practitioner/:2:0","tags":["AWS"],"title":"How to Study for the AWS Certified Cloud Practitioner Exam","uri":"/aws_certified_cloud_practitioner/"},{"categories":["Certifications"],"content":"Whitepapers There‚Äôs nothing like good documentation. These readings are a must, and I would suggest highlighting key concepts, and making notecards for these. I‚Äôve heard that AWS won‚Äôt test much on specifics like pricing, but they will make sure to test understanding of why price would increase based on the service you‚Äôre using. Exam Blueprint AWS Overview - Whitepaper AWS Well-Architected Framework - Whitepaper AWS Pricing - Whitepaper AWS Plans - Support Overview ","date":"2020-10-04","objectID":"/aws_certified_cloud_practitioner/:3:0","tags":["AWS"],"title":"How to Study for the AWS Certified Cloud Practitioner Exam","uri":"/aws_certified_cloud_practitioner/"},{"categories":["Certifications"],"content":"Flashcards I‚Äôm passing along the flashcards that I made and used to study for this exam. It includes bits and pieces of the ACG course notes, and notes from the white papers above. I‚Äôm interested in the Anki Flashcard System, so I might export my flashcards to this platform later. But for now, Cram.com has worked fine. Cram.com Flashcards ","date":"2020-10-04","objectID":"/aws_certified_cloud_practitioner/:4:0","tags":["AWS"],"title":"How to Study for the AWS Certified Cloud Practitioner Exam","uri":"/aws_certified_cloud_practitioner/"},{"categories":["Certifications"],"content":"Celebrate When you pass, you get a shiny certificate, and access to a secret AWS store where you can buy swag such as a small notebook with AWS written on it for $5 üòâ ","date":"2020-10-04","objectID":"/aws_certified_cloud_practitioner/:5:0","tags":["AWS"],"title":"How to Study for the AWS Certified Cloud Practitioner Exam","uri":"/aws_certified_cloud_practitioner/"},{"categories":null,"content":"This post assumes you already have some familiarity with Hugo for building a blog, and you have set up a free-tier AWS account. You don‚Äôt have to be an expert at either Hugo or AWS to follow along with the guide below. Really the only requirement is that you‚Äôre able to follow the Hugo quick start guide. Below is an AWS architecture diagram I created using diagrams.net, the free version of draw.io. This diagram visualizes how my website runs for just $0.50 a month (exc;uding the $12/year domain name registration). While making this, I was a little fuzzy on how Route53 actually worked, so I included some additional detail on how it interfaces with an example ISP. Here are the main steps I‚Äôll talk about in this diagram: Route53 ‚Äì registering a domain, validating it, and routing traffic to Cloudfront Cloudfront ‚Äì mapping your S3 bucket endpoint S3 ‚Äì hosting your blog as a static website Your Code ‚Äì generating a Hugo blog GitHub ‚Äì hosting your code CodeBuild ‚Äì building your Hugo site (CI/CD) ","date":"2020-08-02","objectID":"/build_hugo_on_aws/:0:0","tags":["AWS","Hugo"],"title":"Build a Serverless Hugo Blog on AWS for $0.50 per month","uri":"/build_hugo_on_aws/"},{"categories":null,"content":"Route53 I pay $0.50 per month for one hosted zone on Route53. AWS actually created this hosted zone for me when I purchased my domain through Route53. Once you purchase a domain, you need to obtain the SSL/TLS certificate through ACM to identify the site over the Internet. Here is exactly how you do that. So far you should have 2 DNS records set up from Route53 (NS, SOA) and 1 or more records set up from ACM (CNAMEs). Later you‚Äôll add an A record to route traffic to your Cloudfront distribution for each CNAME record. If you‚Äôre interested in the piece of the diagram focused on connecting to the Internet and the relationship between ISPs and Route53, I learned a lot from How internet traffic is routed to your website or web application. ","date":"2020-08-02","objectID":"/build_hugo_on_aws/:1:0","tags":["AWS","Hugo"],"title":"Build a Serverless Hugo Blog on AWS for $0.50 per month","uri":"/build_hugo_on_aws/"},{"categories":null,"content":"Cloudfront Cloudfront speeds up website load times by storing the build assets at edge locations close to people viewing your blog. AWS wants you to select the Origin Domain Name from a dropdown, but you‚Äôll want to paste in the actual endpoint to your s3 bucket that contains your Hugo build. For example, mine is http://trmccormick.com.s3-website.us-east-2.amazonaws.com/. You‚Äôll be able to leave a lot of the settings in Cloudfront to their defaults. I chose to use automatic object compression, and I changed the price class so that I‚Äôm only using edge locations in North America. You‚Äôll want to map your domain name(s) in the section ‚ÄúAlternate Domain Names (CNAMES).‚Äù I have two: trmccormick.com and www.trmccormick.com, so I just need to go to Route53 and grab those values. Once your distribution has been created and is successfully deployed, you‚Äôll see a Cloudfront distribution for each domain name you added in the last bullet point. In the Route53 section, I mentioned you‚Äôll want to add A records for these Cloudfront domain names. So go over to Route53 and do that so users are re-routed from your domain to a Cloudfront location closest to them. ","date":"2020-08-02","objectID":"/build_hugo_on_aws/:2:0","tags":["AWS","Hugo"],"title":"Build a Serverless Hugo Blog on AWS for $0.50 per month","uri":"/build_hugo_on_aws/"},{"categories":null,"content":"S3 S3 is super easy to figure out: you just upload your /public/ folder that Hugo builds when you run hugo -D on your local machine. Create an s3 bucket with the name of your website (or whatever name you want, actually) Make the entire bucket public. Easiest way to do this is to go to the Permissions section of your S3 bucket and edit the Bucket Policy to look like the following, replacing your bucket name with mine: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowPublicRead\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::trmccormick.com/*\" } ] } Finally upload your /public/ folder created from your local machine If you‚Äôve followed the steps, your blog should start appearing when you visit your domain. It should score pretty highly on Google Page Speed Insights for several reasons. If you go to that site and type in your domain, you‚Äôll see why your website is fast, and how you might be able to speed it up even further. Each time you make any changes to your Hugo site, you‚Äôd need to overwrite all of the files in your S3 bucket. That‚Äôs fine if you want to keep it pretty simple. If you‚Äôre creating a lot of content or making frequent changes to your Hugo site, you probably want to work through the next sections that help you automate that task using GitHub and CodeBuild. ","date":"2020-08-02","objectID":"/build_hugo_on_aws/:3:0","tags":["AWS","Hugo"],"title":"Build a Serverless Hugo Blog on AWS for $0.50 per month","uri":"/build_hugo_on_aws/"},{"categories":null,"content":"Hugo I won‚Äôt tell you how you should organize your Hugo build, but there is one specific thing you‚Äôll need to add into your git repository: a build spec. This is basically an instructions file. With CodeBuild, you‚Äôll be telling a fresh computer how to build your site. There will be modules it needs to install, files it needs to build, and it will need to know what to do with the files it builds. Below is my buildspec.yml file in the root directory of my code repository. version:0.2phases:install:commands:- echo Entered the install phase...- yum install curl- yum install asciidoctor -y- curl -s -L https://github.com/gohugoio/hugo/releases/download/v0.80.0/hugo_0.80.0_Linux-64bit.deb -o hugo.deb- dpkg -i hugo.debfinally:- echo Installation donebuild:commands:- echo Building ...- echo Build started on `date`- cd $CODEBUILD_SRC_DIR- hugo --quiet- aws s3 sync --delete public/ s3://your-bucket- aws cloudfront create-invalidation --distribution-id xxx --paths '/*'finally:- echo Build finishedartifacts:files:- '**/*'base-directory:$CODEBUILD_SRC_DIR/publicdiscard-paths:no Three things to note here: We‚Äôre using commands yum install because we‚Äôll be using AWS Linux as our environment machine in CodeBuild. If you run into trouble here, it‚Äôs because you‚Äôve selected a different build environment in CodeBuild. You‚Äôll want to change the s3:// location to reflect your bucket name. This line sends the contents inside of the /public/ folder from your build environment to your s3 bucket, deleting whatever is currently in the bucket. You‚Äôll need to create a Cloudfront invalidation across the edge locations where your site has been downloaded. This basically just means you‚Äôre clearing everyone‚Äôs cache right now, instead of waiting for the cache to expire. ","date":"2020-08-02","objectID":"/build_hugo_on_aws/:4:0","tags":["AWS","Hugo"],"title":"Build a Serverless Hugo Blog on AWS for $0.50 per month","uri":"/build_hugo_on_aws/"},{"categories":null,"content":"GitHub If you‚Äôve never used GitHub, it‚Äôs super simple to set up. You‚Äôll create an account, set up a new repository (name it something creative like‚Ä¶ blog). I‚Äôll let GitHub explain the rest. ","date":"2020-08-02","objectID":"/build_hugo_on_aws/:5:0","tags":["AWS","Hugo"],"title":"Build a Serverless Hugo Blog on AWS for $0.50 per month","uri":"/build_hugo_on_aws/"},{"categories":null,"content":"CodeBuild In CodeBuild, you‚Äôre able to set up a webhook to your GitHub repository, which essentially means GitHub is sending a notifcation to CodeBuild every time code is pushed to the main branch. First, configure your Source to be your GitHub repository. Second, change your Environment OS to Amazon Linux 2. It can be a standard runtime with the most recent image. Third, you‚Äôll want to create a new service role. More on that in a second. Last, you probably want to activate CloudWatch logs so you can see log output of the build. This is necessary for debugging. You‚Äôll be able to quickly isolate problems and fix them in AWS or in your buildspec. Okay back to the service role. CodeBuild needs to work with s3 and Cloudfront, so you‚Äôll have to go to IAM and attach two new policies to this role: The first policy is related to s3. You‚Äôll see in the buildspec.yml file that we‚Äôre executing the S3 sync command with the AWS CLI. Your bucket policy already has public read access, but you need to make a policy that gives access to delete objects from that bucket. Here is what my json looks like: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Resource\": [ \"arn:aws:s3:::trmccormick.com/*\", ], \"Sid\": \"s3_sync\", \"Effect\": \"Allow\", \"Action\": [ \"s3:DeleteObject\", \"s3:GetBucketLocation\", \"s3:GetObject\", \"s3:ListBucket\", \"s3:PutObject\", \"s3:PutObjectAcl\", \"s3:ListObjects\" ] } ] } The second policy is related to Cloudfront. In the buildspec.yml file, we created a Cloudfront invaldation, so we‚Äôll need to give access to CodeBuild to be able to execute that. Change your resource to your Cloudfront distribution ARN and you should be all set. Here is the json for that policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"cloudfront_invalidations\", \"Effect\": \"Allow\", \"Action\": [ \"cloudfront:ListInvalidations\", \"cloudfront:GetInvalidation\", \"cloudfront:CreateInvalidation\" ], \"Resource\": \"arn:aws:cloudfront::####:distribution/xxxxx\" } ] } That‚Äôs it, now you should be able to push code to your GitHub repo and CodeBuild will run through the buildspec, and your site should reflect changes in just a matter of minutes. I‚Äôve been running my website since 2019 using this serverless strategy. Here is a picture of my monthly bill. I‚Äôll probably always stay near $0.51 per month. Thanks for reading! ","date":"2020-08-02","objectID":"/build_hugo_on_aws/:6:0","tags":["AWS","Hugo"],"title":"Build a Serverless Hugo Blog on AWS for $0.50 per month","uri":"/build_hugo_on_aws/"}]