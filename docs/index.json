[{"categories":null,"content":"Are you frustrated that your Hugo website builds are not reflecting your local changes? If you‚Äôre using scss files to manage your website styling, and the changes you make locally are not showing up on web server, then you might be facing challenges installing Hugo Extended. Here are some common errors you might see in AWS CloudWatch: hugo: /lib64/libm.so.6: version 'GLIBC_2.29' not found (required by hugo) hugo: /lib64/libstdc++.so.6: version 'GLIBCXX_3.4.26' not found (required by hugo) tpl/internal/go_templates/texttemplate/helper.go:11:2: package io/fs is not in GOROOT (/root/.goenv/versions/1.14.12/src/io/fs) To resolve these errors, you need to do three things within your buildspec: Download a specific version of Go that will work with Hugo Extended Adjust your web server environment variables to use a specific version of Go Using Go to install Hugo Extended Here is the buildspec I‚Äôm using for my site, so feel free to copy the install commands for your own use. version:0.2phases:install:commands:- echo Entered the install phase...- wget https://golang.org/dl/go1.16.7.linux-amd64.tar.gz#installs Go 1.16.7- tar -xzf go1.16.7.linux-amd64.tar.gz#extracts gzipped archive file- mv go /usr/local #moves go to /usr/local- export GOROOT=/usr/local/go#adds go to env variable- export PATH=$GOPATH/bin:$GOROOT/bin:$PATH #adds go to path- go version#print version of go to console- yum install asciidoctor -y#my hugo template needs asciidoctor- mkdir $HOME/src#make new src dir- cd $HOME/src#go to src dir- git clone https://github.com/gohugoio/hugo.git#clone hugo- cd hugo#go to hugo dir- go install --tags extended#use go to install extended hugofinally:- echo Installation donebuild:commands:- echo Building...- echo Build started on `date`- cd $CODEBUILD_SRC_DIR- hugo --quiet- aws s3 sync --delete docs/ s3://trmccormick.com- aws cloudfront create-invalidation --distribution-id **** --paths '/*'finally:- echo Build finishedartifacts:files:- '**/*'base-directory:$CODEBUILD_SRC_DIR/docsdiscard-paths:no ","date":"2021-09-05","objectID":"/hugo_extended/:0:0","tags":["AWS. Hugo"],"title":"Buildspec for Hugo Extended on Amazon Linux 2","uri":"/hugo_extended/"},{"categories":null,"content":"Trevor McCormick","date":"2021-08-01","objectID":"/about/","tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":"This website was designed to publish thoughts, how-to‚Äôs, resources, notes, and everything in-between in the general realm of data products, but also whatever comes to mind. I live in Wyandotte, Michigan with my wife, Haley, and our dog, Captain. I‚Äôm working as a Product Analyst for Disney Streaming, working within the Data Solutions \u0026 Instrumentation team to design and implement tagging across all of Disney‚Äôs streaming products. ","date":"2021-08-01","objectID":"/about/:0:0","tags":null,"title":"About Me","uri":"/about/"},{"categories":["Certifications"],"content":"The AWS Certified Solutions Architect Associate Exam tests your ability to translate specific client needs into basic cloud architectural designs. While the AWS Cerified Cloud Practitioner Exam covers the core concepts of AWS at a broad level, the Solutions Architect Associate Exam goes a level deeper in all areas. You should have hands-on experience designing end-to-end solutions, deploying VPCs that support various networking, compute, storage, and database strategies. ","date":"2021-06-02","objectID":"/aws_certified_solutions_architect_associate/:0:0","tags":["AWS"],"title":"Preparing for the AWS Certified Solutions Architect Associate Exam","uri":"/aws_certified_solutions_architect_associate/"},{"categories":["Certifications"],"content":"Background There is no better preparation than hands-on experience with AWS. Before I attempted this certification exam, I had about three years of experience working across AWS, primarily using services to build my website and handle data ingestion and warehousing processes at work. I had drawn out some simple architecture documents up to this point, and I could generally describe what was happening. However, I knew that in order to pass the exam, I would need to strengthen my understanding of services I was already familiar with (S3, RDS, IAM), and spend a significant amount of time learning the ones I did not use day to day (VPC, Route53, Networking, etc). Here is a link to the exam guide which identifies the four content domains you must know in order to pass: resiliency, high-performance, security, and cost-optimization. When you study for this exam, if you‚Äôre reading about a specific service, or building something out in the console, try to understand the service offering with these four concepts in mind. Some key questions to ask yourself might be: what happens if this service fails? how can I scale my application to meet demand? how can I lockdown the components of my VPC so that it is protected from internal and external threats? is this the most cost-effective way to achieve my goals? ","date":"2021-06-02","objectID":"/aws_certified_solutions_architect_associate/:1:0","tags":["AWS"],"title":"Preparing for the AWS Certified Solutions Architect Associate Exam","uri":"/aws_certified_solutions_architect_associate/"},{"categories":["Certifications"],"content":"A Cloud Guru I felt like the first best step for me would be to get through all of the videos and exercises in A Cloud Guru‚Äôs - AWS Certified Solutions Architect Associate SAA-C02 course. There was 45 hours of content in this course alone, so it was definitely a big first step. As I was going through this course, I browsed through some of the other offerings, and found the AWS Well-Architected Framework course fairly helpful. What I found most helpful was the hands-on labs. Here is a curated list of labs I found most helpful: ALBs and Auto-Scaling AMIs Bastion Hosts Database Migration EBS Volumes RDS SQS VPCs ","date":"2021-06-02","objectID":"/aws_certified_solutions_architect_associate/:2:0","tags":["AWS"],"title":"Preparing for the AWS Certified Solutions Architect Associate Exam","uri":"/aws_certified_solutions_architect_associate/"},{"categories":["Certifications"],"content":"FAQs, Cheat Sheets, and Tutorials After taking the two ACG courses and moving through these labs, I used the following resources to prepare for the ACG practice exams: AWS Cheat Sheets by Tutorials Dojo AWS FAQs AWS 200-level Hands-On Tutorials AWS Reference Architecture Diagrams After all of this preparation, my wife basically told me I needed to stop studying and just take the exam üòÖ So that‚Äôs what I did! ","date":"2021-06-02","objectID":"/aws_certified_solutions_architect_associate/:3:0","tags":["AWS"],"title":"Preparing for the AWS Certified Solutions Architect Associate Exam","uri":"/aws_certified_solutions_architect_associate/"},{"categories":["Certifications"],"content":"Celebrate You should have a great cloud foundation if you dove into the above resources and put in the work. You can comfortably build a VPC from scratch, you can start tinkering with your own cloud solutions, and you should be able to understand and contribute to your team‚Äôs cloud strategy, even if your team uses another cloud provider. Although I do not think will proceed studying for the Solutions Architect Professional exam, I do want to improve my ability to set up cloud solutions for clients. I‚Äôm planning to learn more about tools like CloudFormation, Elastic Beanstalk, and all of the analytics services. I‚Äôm currently on the learning path through ACG to take the Data and Analytics specialty exam in the next couple of years. ","date":"2021-06-02","objectID":"/aws_certified_solutions_architect_associate/:4:0","tags":["AWS"],"title":"Preparing for the AWS Certified Solutions Architect Associate Exam","uri":"/aws_certified_solutions_architect_associate/"},{"categories":["Data"],"content":"In this post I‚Äôll walk through: how to create a public PostgreSQL instance in RDS using free-tier how to create a table and load it with data from a csv file how to query data from that table using psycopg2 ","date":"2021-02-04","objectID":"/aws_rds_postgres_python/:0:0","tags":["Python","AWS"],"title":"How To Query PostgreSQL in AWS RDS with Python","uri":"/aws_rds_postgres_python/"},{"categories":["Data"],"content":"PostgreSQL on RDS The only thing you have to do in the AWS console is create a Postgres DB instance and make sure it is open to the public (just for this example). Here is how to do that: Go to Databases in RDS, and choose the region you want to create a database instance Create a database, selecting ‚ÄúStandard Create‚Äù, and the PostgreSQL configuration. Make sure to use free-tier. You can name the database anything you want, and choose a username and password. The most important step is in ‚ÄúConnectivity‚Äù: make sure to fill in the bubble for ‚ÄúYes‚Äù to Public Access. If you‚Äôre using this database for any real-life work, then you‚Äôll want to fill in ‚ÄúNo‚Äù. You‚Äôll have to do some work to configure security groups and look at your architecture to only allow connections you want to approve if that is the case. Once the database has been created, you‚Äôll be able to find the database endpoint in the ‚ÄúConnectivity \u0026 security‚Äù section. You‚Äôll use that to create a json file with your credentials, which should look something like this: { \"user\":\"postgres\", \"password\":\"password\", \"database\":\"postgres\", \"host\":\"xxxx.xxxxxxx.us-east-2.rds.amazonaws.com\" } ","date":"2021-02-04","objectID":"/aws_rds_postgres_python/:1:0","tags":["Python","AWS"],"title":"How To Query PostgreSQL in AWS RDS with Python","uri":"/aws_rds_postgres_python/"},{"categories":["Data"],"content":"Query with Python So you‚Äôve set up a Postgres DB instance, but there is no data in it. We‚Äôll need to connect to the instance and load data with Python. To go forward with this exercise, you‚Äôll need pip, and you‚Äôll need to install the follwing packages: psycopg2-binary, pandas, and sqlalchemy. I‚Äôve provided some example functions that you can use to get started. Here is a quick summary of the sections, with the actual python code at the bottom of the post. Import: so you can skip a bunch of database driver steps Client: to connect to the psql instance for queries Load: to load data into psql. I only put one function in this class for an example, so you can create and load a table in one step. Query: to query data in a table within your DB instance Meta: to inspect the DB instance ","date":"2021-02-04","objectID":"/aws_rds_postgres_python/:2:0","tags":["Python","AWS"],"title":"How To Query PostgreSQL in AWS RDS with Python","uri":"/aws_rds_postgres_python/"},{"categories":["Data"],"content":"Google Colab Most of my ad-hoc work is done in Google Colab because it‚Äôs easy to run code blocks and debug interactively. I‚Äôm going to share an example Colab Notebook with you so you should be up and running fast. Here is the link to the Python Notebook that you can upload to your own Colab environment. In the Google Colab environment, you need to upload two files: your credentials json file, and your dataset. In this case, I‚Äôve downloaded the iris dataset and I will upload it to my DB instance as the table iris. ","date":"2021-02-04","objectID":"/aws_rds_postgres_python/:3:0","tags":["Python","AWS"],"title":"How To Query PostgreSQL in AWS RDS with Python","uri":"/aws_rds_postgres_python/"},{"categories":["Data"],"content":"Video Walkthrough Below is a screen recording of me going through the Colab process. I‚Äôm uploading two files, and running through all the code blocks to connect to my DB instance and work with it. And here is the Python code you can copy and try out for yourself: ","date":"2021-02-04","objectID":"/aws_rds_postgres_python/:3:1","tags":["Python","AWS"],"title":"How To Query PostgreSQL in AWS RDS with Python","uri":"/aws_rds_postgres_python/"},{"categories":["Certifications"],"content":"The AWS Certified Cloud Practitioner Exam challenges your knowledge of introductory cloud concepts. The exam does not cover any programming, any system admin types of tasks, or any advanced solution architecture designs. Rather, it tests your understanding of the mechanics, the scope, and the benefits of cloud computing, as well as the AWS philosophy in general. ","date":"2020-10-04","objectID":"/aws_certified_cloud_practitioner/:0:0","tags":["AWS"],"title":"Preparing for the AWS Certified Cloud Practitioner Exam","uri":"/aws_certified_cloud_practitioner/"},{"categories":["Certifications"],"content":"Computing Foundations I recommend browsing through these two courses to evaluate your knowledge of how computers work, from doing basic tasks to coordinating and executing complex operations with other machines. Stanford CS 101 - Introduction to Computing Principles Harvard CS50 - Introduction to Computer Science This course features one of my favorite professors of all time ‚Äì David Malan. Once you‚Äôre finished with these courses, you will be able to explain how computers work, how the Internet works, and why cloud computing provides such a tremendous value. ","date":"2020-10-04","objectID":"/aws_certified_cloud_practitioner/:1:0","tags":["AWS"],"title":"Preparing for the AWS Certified Cloud Practitioner Exam","uri":"/aws_certified_cloud_practitioner/"},{"categories":["Certifications"],"content":"Guided Training Once you‚Äôre confident about how computers work, you‚Äôre ready to jump in to learning about the cloud. There are two cloud training platforms I will recommend for this certification: A Cloud Guru and Cloud Academy. A Cloud Guru - AWS Certified Cloud Practitioner 2020 Designed for the non-technical crowd, this course provides 20 hours of videos, several quizzes along the way, and a final practice exam. The instructor Ryan Kroonenburg is phenomenal. I used this platform exclusively, along with the whitepapers below, and I passed my exam. A Cloud Guru‚Äôs current pricing is $32/month, and it includes access to all learning paths. I‚Äôve found it extremely worthwhile, and I‚Äôve been fortunate to use some education reimbursement funds from my employer to cover the cost for the full year. There is also a Personal Plus plan, which I have not used, but I do see the benefit in having your own managed cloud sandbox. Cloud Academy - AWS Certified Cloud Practitioner Preparation Another cloud platform out of London, this is a popular learning recommendation. I have no personal experience with this platform, but you can check out their free 7-day trial to see if you like it better than A Cloud Guru. Their pricing is about 50% more expensive at $49/month. ","date":"2020-10-04","objectID":"/aws_certified_cloud_practitioner/:2:0","tags":["AWS"],"title":"Preparing for the AWS Certified Cloud Practitioner Exam","uri":"/aws_certified_cloud_practitioner/"},{"categories":["Certifications"],"content":"Whitepapers Once you‚Äôve taken a formal course that gets you doing actual exercises in the AWS platfom, it‚Äôs time to grind through some documentation. These readings are a must, and I would suggest highlighting key concepts, and making notecards for these. I‚Äôve heard that AWS won‚Äôt test much on specifics like pricing, but they will make sure to test understanding of why price would increase based on the service you‚Äôre using. Exam Blueprint AWS Overview - Whitepaper AWS Well-Architected Framework - Whitepaper AWS Pricing - Whitepaper AWS Plans - Support Overview ","date":"2020-10-04","objectID":"/aws_certified_cloud_practitioner/:3:0","tags":["AWS"],"title":"Preparing for the AWS Certified Cloud Practitioner Exam","uri":"/aws_certified_cloud_practitioner/"},{"categories":["Certifications"],"content":"Flashcards Lastly, I‚Äôm passing along the flashcards that I made and used to study for this exam. It includes bits and pieces of the ACG course notes, and notes from the white papers above. I‚Äôm interested in the Anki Flashcard System, so I might export my flashcards to this platform later. But for now, Cram.com has worked fine. Cram.com Flashcards ","date":"2020-10-04","objectID":"/aws_certified_cloud_practitioner/:4:0","tags":["AWS"],"title":"Preparing for the AWS Certified Cloud Practitioner Exam","uri":"/aws_certified_cloud_practitioner/"},{"categories":["Certifications"],"content":"Take A Practice Test If you‚Äôve completed the steps above, you should be ready to take some practice tests! There are several practice tests on the Internet that you can take for \u003c$20. If you‚Äôve enrolled in one of the courses above, they should include a practice test in your subscription. Otherwise, there are individual practice tests you can purchase from somewhere like WhizLabs or Udemy. ","date":"2020-10-04","objectID":"/aws_certified_cloud_practitioner/:5:0","tags":["AWS"],"title":"Preparing for the AWS Certified Cloud Practitioner Exam","uri":"/aws_certified_cloud_practitioner/"},{"categories":["Certifications"],"content":"Celebrate When you pass, you get a shiny certificate, and access to a secret AWS store where you can buy swag such as a small notebook with AWS written on it for $5 üòâ Congrats ‚Äì the next step is to teach others how you passed! ","date":"2020-10-04","objectID":"/aws_certified_cloud_practitioner/:5:1","tags":["AWS"],"title":"Preparing for the AWS Certified Cloud Practitioner Exam","uri":"/aws_certified_cloud_practitioner/"},{"categories":null,"content":"This post assumes you already have some familiarity with Hugo for building a blog, and you have set up a free-tier AWS account. You don‚Äôt have to be an expert at either Hugo or AWS to follow along with the guide below. Really the only requirement is that you‚Äôre able to follow the Hugo quick start guide. Below is an AWS architecture diagram I created using diagrams.net, the free version of draw.io. This diagram visualizes how my website runs for just $0.50 a month (exc;uding the $12/year domain name registration). While making this, I was a little fuzzy on how Route53 actually worked, so I included some additional detail on how it interfaces with an example ISP. Here are the main steps I‚Äôll talk about in this diagram: Route53 ‚Äì registering a domain, validating it, and routing traffic to Cloudfront Cloudfront ‚Äì mapping your S3 bucket endpoint S3 ‚Äì hosting your blog as a static website Your Code ‚Äì generating a Hugo blog GitHub ‚Äì hosting your code CodeBuild ‚Äì building your Hugo site (CI/CD) ","date":"2020-08-02","objectID":"/build_hugo_on_aws/:0:0","tags":["AWS","Hugo"],"title":"Build a Serverless Hugo Blog on AWS for $0.50 per month","uri":"/build_hugo_on_aws/"},{"categories":null,"content":"Route53 I pay $0.50 per month for one hosted zone on Route53. AWS actually created this hosted zone for me when I purchased my domain through Route53. Once you purchase a domain, you need to obtain the SSL/TLS certificate through ACM to identify the site over the Internet. Here is exactly how you do that. So far you should have 2 DNS records set up from Route53 (NS, SOA) and 1 or more records set up from ACM (CNAMEs). Later you‚Äôll add an A record to route traffic to your Cloudfront distribution for each CNAME record. If you‚Äôre interested in the piece of the diagram focused on connecting to the Internet and the relationship between ISPs and Route53, I learned a lot from How internet traffic is routed to your website or web application. ","date":"2020-08-02","objectID":"/build_hugo_on_aws/:1:0","tags":["AWS","Hugo"],"title":"Build a Serverless Hugo Blog on AWS for $0.50 per month","uri":"/build_hugo_on_aws/"},{"categories":null,"content":"Cloudfront Cloudfront speeds up website load times by storing the build assets at edge locations close to people viewing your blog. AWS wants you to select the Origin Domain Name from a dropdown, but you‚Äôll want to paste in the actual endpoint to your s3 bucket that contains your Hugo build. For example, mine is http://trmccormick.com.s3-website.us-east-2.amazonaws.com/. You‚Äôll be able to leave a lot of the settings in Cloudfront to their defaults. I chose to use automatic object compression, and I changed the price class so that I‚Äôm only using edge locations in North America. You‚Äôll want to map your domain name(s) in the section ‚ÄúAlternate Domain Names (CNAMES).‚Äù I have two: trmccormick.com and www.trmccormick.com, so I just need to go to Route53 and grab those values. Once your distribution has been created and is successfully deployed, you‚Äôll see a Cloudfront distribution for each domain name you added in the last bullet point. In the Route53 section, I mentioned you‚Äôll want to add A records for these Cloudfront domain names. So go over to Route53 and do that so users are re-routed from your domain to a Cloudfront location closest to them. ","date":"2020-08-02","objectID":"/build_hugo_on_aws/:2:0","tags":["AWS","Hugo"],"title":"Build a Serverless Hugo Blog on AWS for $0.50 per month","uri":"/build_hugo_on_aws/"},{"categories":null,"content":"S3 S3 is super easy to figure out: you just upload your /public/ folder that Hugo builds when you run hugo -D on your local machine. Create an s3 bucket with the name of your website (or whatever name you want, actually) Make the entire bucket public. Easiest way to do this is to go to the Permissions section of your S3 bucket and edit the Bucket Policy to look like the following, replacing your bucket name with mine: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowPublicRead\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::trmccormick.com/*\" } ] } Finally upload your /public/ folder created from your local machine If you‚Äôve followed the steps, your blog should start appearing when you visit your domain. It should score pretty highly on Google Page Speed Insights for several reasons. If you go to that site and type in your domain, you‚Äôll see why your website is fast, and how you might be able to speed it up even further. Each time you make any changes to your Hugo site, you‚Äôd need to overwrite all of the files in your S3 bucket. That‚Äôs fine if you want to keep it pretty simple. If you‚Äôre creating a lot of content or making frequent changes to your Hugo site, you probably want to work through the next sections that help you automate that task using GitHub and CodeBuild. ","date":"2020-08-02","objectID":"/build_hugo_on_aws/:3:0","tags":["AWS","Hugo"],"title":"Build a Serverless Hugo Blog on AWS for $0.50 per month","uri":"/build_hugo_on_aws/"},{"categories":null,"content":"Hugo I won‚Äôt tell you how you should organize your Hugo build, but there is one specific thing you‚Äôll need to add into your git repository: a build spec. This is basically an instructions file. With CodeBuild, you‚Äôll be telling a fresh computer how to build your site. There will be modules it needs to install, files it needs to build, and it will need to know what to do with the files it builds. Below is my buildspec.yml file in the root directory of my code repository. version:0.2phases:install:commands:- echo Entered the install phase...- yum install curl- yum install asciidoctor -y- curl -s -L https://github.com/gohugoio/hugo/releases/download/v0.80.0/hugo_0.80.0_Linux-64bit.deb -o hugo.deb- dpkg -i hugo.debfinally:- echo Installation donebuild:commands:- echo Building ...- echo Build started on `date`- cd $CODEBUILD_SRC_DIR- hugo --quiet- aws s3 sync --delete public/ s3://your-bucket- aws cloudfront create-invalidation --distribution-id xxx --paths '/*'finally:- echo Build finishedartifacts:files:- '**/*'base-directory:$CODEBUILD_SRC_DIR/publicdiscard-paths:no Three things to note here: We‚Äôre using commands yum install because we‚Äôll be using AWS Linux as our environment machine in CodeBuild. If you run into trouble here, it‚Äôs because you‚Äôve selected a different build environment in CodeBuild. You‚Äôll want to change the s3:// location to reflect your bucket name. This line sends the contents inside of the /public/ folder from your build environment to your s3 bucket, deleting whatever is currently in the bucket. You‚Äôll need to create a Cloudfront invalidation across the edge locations where your site has been downloaded. This basically just means you‚Äôre clearing everyone‚Äôs cache right now, instead of waiting for the cache to expire. ","date":"2020-08-02","objectID":"/build_hugo_on_aws/:4:0","tags":["AWS","Hugo"],"title":"Build a Serverless Hugo Blog on AWS for $0.50 per month","uri":"/build_hugo_on_aws/"},{"categories":null,"content":"GitHub If you‚Äôve never used GitHub, it‚Äôs super simple to set up. You‚Äôll create an account, set up a new repository (name it something creative like‚Ä¶ blog). I‚Äôll let GitHub explain the rest. ","date":"2020-08-02","objectID":"/build_hugo_on_aws/:5:0","tags":["AWS","Hugo"],"title":"Build a Serverless Hugo Blog on AWS for $0.50 per month","uri":"/build_hugo_on_aws/"},{"categories":null,"content":"CodeBuild In CodeBuild, you‚Äôre able to set up a webhook to your GitHub repository, which essentially means GitHub is sending a notifcation to CodeBuild every time code is pushed to the main branch. First, configure your Source to be your GitHub repository. Second, change your Environment OS to Amazon Linux 2. It can be a standard runtime with the most recent image. Third, you‚Äôll want to create a new service role. More on that in a second. Last, you probably want to activate CloudWatch logs so you can see log output of the build. This is necessary for debugging. You‚Äôll be able to quickly isolate problems and fix them in AWS or in your buildspec. Okay back to the service role. CodeBuild needs to work with s3 and Cloudfront, so you‚Äôll have to go to IAM and attach two new policies to this role: The first policy is related to s3. You‚Äôll see in the buildspec.yml file that we‚Äôre executing the S3 sync command with the AWS CLI. Your bucket policy already has public read access, but you need to make a policy that gives access to delete objects from that bucket. Here is what my json looks like: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Resource\": [ \"arn:aws:s3:::trmccormick.com/*\", ], \"Sid\": \"s3_sync\", \"Effect\": \"Allow\", \"Action\": [ \"s3:DeleteObject\", \"s3:GetBucketLocation\", \"s3:GetObject\", \"s3:ListBucket\", \"s3:PutObject\", \"s3:PutObjectAcl\", \"s3:ListObjects\" ] } ] } The second policy is related to Cloudfront. In the buildspec.yml file, we created a Cloudfront invaldation, so we‚Äôll need to give access to CodeBuild to be able to execute that. Change your resource to your Cloudfront distribution ARN and you should be all set. Here is the json for that policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"cloudfront_invalidations\", \"Effect\": \"Allow\", \"Action\": [ \"cloudfront:ListInvalidations\", \"cloudfront:GetInvalidation\", \"cloudfront:CreateInvalidation\" ], \"Resource\": \"arn:aws:cloudfront::####:distribution/xxxxx\" } ] } That‚Äôs it, now you should be able to push code to your GitHub repo and CodeBuild will run through the buildspec, and your site should reflect changes in just a matter of minutes. I‚Äôve been running my website since 2019 using this serverless strategy. Here is a picture of my monthly bill. I‚Äôll probably always stay near $0.51 per month. Thanks for reading! ","date":"2020-08-02","objectID":"/build_hugo_on_aws/:6:0","tags":["AWS","Hugo"],"title":"Build a Serverless Hugo Blog on AWS for $0.50 per month","uri":"/build_hugo_on_aws/"}]